{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ad38cc-dd43-4844-ba3a-a47d97fc6d1a",
   "metadata": {},
   "source": [
    "## Section C - Question no 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff4da8-fa27-4901-a30a-bea00266edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import min, max, avg, col, count\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"Startup Analysis\").getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "df = spark.read.csv(\"StartupData.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Register for SQL queries\n",
    "df.createOrReplaceTempView(\"startups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573449a9-01ee-4858-9bde-99ff9b8db51a",
   "metadata": {},
   "source": [
    "1.Overall minimum, maximum, and average profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba44017-80c6-413d-959a-7771c05b481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using sparkSql\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(Profit) AS Min_Profit, \n",
    "        MAX(Profit) AS Max_Profit, \n",
    "        AVG(Profit) AS Avg_Profit\n",
    "    FROM startups\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb210323-8cac-4ff0-85db-4b02c4186566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using pySpark\n",
    "df.select(\n",
    "    min(\"Profit\").alias(\"Min_Profit\"),\n",
    "    max(\"Profit\").alias(\"Max_Profit\"),\n",
    "    avg(\"Profit\").alias(\"Avg_Profit\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df92534-403f-4044-9528-94586d53ff86",
   "metadata": {},
   "source": [
    "2. State with the most startups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55069a-928b-4ff8-b2fd-a11774849323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using sparkSql\n",
    "spark.sql(\"\"\"\n",
    "    SELECT State, COUNT(*) AS Num_Startups\n",
    "    FROM startups\n",
    "    GROUP BY State\n",
    "    ORDER BY Num_Startups DESC\n",
    "    LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024491ec-0c02-49de-a10f-067891bf9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using pySpark\n",
    "df.groupBy(\"State\") \\\n",
    "  .count() \\\n",
    "  .orderBy(col(\"count\").desc()) \\\n",
    "  .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4273d-3308-4d99-ac2c-bcc985fc14e0",
   "metadata": {},
   "source": [
    "3. Average administration cost for startups in Florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2addf77-71ac-4cb0-9d9b-d1960c43607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using sparkSql\n",
    "spark.sql(\"\"\"\n",
    "    SELECT AVG(Administration) AS Avg_Admin_Cost\n",
    "    FROM startups\n",
    "    WHERE State = 'Florida'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00263ea3-4ea1-46ef-8057-789b5ea2d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using pySpark\n",
    "df.filter(col(\"State\") == \"Florida\") \\\n",
    "  .select(avg(\"Administration\").alias(\"Avg_Admin_Cost\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1b5e83-2908-4b56-a3af-f15d426aa298",
   "metadata": {},
   "source": [
    "4. How many startups are located in California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c12bd-f56b-496b-999a-6e9e991fa053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using sparkSql\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS Num_Startups_CA\n",
    "    FROM startups\n",
    "    WHERE State = 'California'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af2b94-3cea-46be-9c2c-7d40bb09c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using pySpark\n",
    "df.filter(col(\"State\") == \"California\") \\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb8c97-c8eb-413c-83e2-fc37f873b9e4",
   "metadata": {},
   "source": [
    "5.Startups with profit > 100000 and marketing spend < 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a1886-93ab-44a6-adb0-2da439266834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using sparkSql\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(*) AS Num_Target_Startups\n",
    "    FROM startups\n",
    "    WHERE Profit > 100000 AND Marketing_Spend < 50000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c61a90-7943-4216-a177-0a873e473db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using pySpark\n",
    "df.filter((col(\"Profit\") > 100000) & (col(\"Marketing_Spend\") < 50000)) \\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f10ef93-6dd4-4c4b-8e47-cb26529b1d85",
   "metadata": {},
   "source": [
    "6. Remove sl_no column and rows with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b9677-5730-4c28-ace8-e419182beb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solved using sparkSql\n",
    "\n",
    "# Drop 'sl_no' column\n",
    "df_cleaned = df.drop('sl_no')\n",
    "\n",
    "# Remove rows with any null values\n",
    "df_cleaned = df_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4659da-316c-4494-bde2-bf1b57cc6e17",
   "metadata": {},
   "source": [
    "## Section C - Question no 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab74d3d-ba67-4acb-b5ef-0218a657ee84",
   "metadata": {},
   "source": [
    "1. Use StringIndexer for string-type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1ec1e7-8b78-43bb-9b65-bd7c1931a27f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringIndexer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Identify string columns\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m string_cols \u001b[38;5;241m=\u001b[39m [field\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mfields \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStringType\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Apply StringIndexer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m string_cols:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import StringType\n",
    "# Identify string columns\n",
    "#string_cols = [field.name for field in df_cleaned.schema.fields if str(field.dataType) == 'string']\n",
    "string_cols = [field.name for field in df_cleaned.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "print(string_cols)\n",
    "# Apply StringIndexer\n",
    "for col in string_cols:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col + \"_indexed\")\n",
    "    df_cleaned = indexer.fit(df_cleaned).transform(df_cleaned).drop(col).withColumnRenamed(col + \"_indexed\", col)\n",
    "\n",
    "# Confirm no string columns\n",
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd61828-d109-4666-94b9-cb5808ae54eb",
   "metadata": {},
   "source": [
    "2. Use VectorAssembler to create features column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c338f4-f7c6-462e-a03d-2fa2930f82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Exclude the target column 'Profit'\n",
    "feature_cols = [col for col in df_cleaned.columns if col != \"Profit\"]\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_cleaned = assembler.transform(df_cleaned).select(\"features\", \"Profit\")\n",
    "\n",
    "df_cleaned.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1262ffe1-0dec-4893-b6b0-a6182e3cc30a",
   "metadata": {},
   "source": [
    "3. Split the data into training (75%) and test (25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebcd900-f288-4546-9b10-d9b73fca652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale if required \n",
    "#scaler = StandardScaler(inputCol=\"features_us\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "#scaler_model = scaler.fit(df_vector)\n",
    "#df_scaled = scaler_model.transform(df_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e1450-92ea-46f7-a959-ded74427c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df_cleaned.randomSplit([0.75, 0.25], seed=42)\n",
    "train_df.count()\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769bef01-5a95-49bb-9bbb-b31f84138a2b",
   "metadata": {},
   "source": [
    "4. Train a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4b97e-10d6-4358-9587-0a72bf08d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Profit\")\n",
    "lr_model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37720f-4221-487c-8880-2cd21021bd1e",
   "metadata": {},
   "source": [
    "5. Predict and print Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673682c-02e6-4be6-aa55-62d46324ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions = lr_model.transform(test_df)\n",
    "evaluator = RegressionEvaluator(labelCol=\"Profit\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3d3f8-8e45-44b4-9f39-f61d848034c7",
   "metadata": {},
   "source": [
    "6. Calculate Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59156ecb-6d99-4dc6-b3e9-ec5e05be3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_evaluator = RegressionEvaluator(labelCol=\"Profit\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
