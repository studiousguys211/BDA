{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b3df92-3ed2-4b8d-9534-9c7d0896081f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q1 - Section C - Feb 2025\n",
    "\n",
    "## from pyspark.sql import SparkSession\n",
    "# Create Spark session\n",
    "## spark = SparkSession.builder.appName(\"PlacementAnalysis\").getOrCreate()\n",
    "\n",
    "mba_place = spark.read.load(\n",
    "    \"dbfs:/FileStore/tables/SparkSQL/mba_placement.csv\",\n",
    "    format=\"csv\",\n",
    "    sep=\",\",\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d5abc2-28df-4d1b-8442-bd9431f3f558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 215"
     ]
    }
   ],
   "source": [
    "mba_place.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad66b2ec-b6e7-4e5a-9b96-2b4e779da0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- ssc_b: string (nullable = true)\n",
      " |-- hsc_p: double (nullable = true)\n",
      " |-- hsc_b: string (nullable = true)\n",
      " |-- hsc_s: string (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- degree_t: string (nullable = true)\n",
      " |-- workex: string (nullable = true)\n",
      " |-- etest_p: double (nullable = true)\n",
      " |-- specialisation: string (nullable = true)\n",
      " |-- mba_p: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mba_place.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f85d37cc-a48c-40cb-a97b-d17f9cbe03a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-----+-------+--------+--------+---------+------+-------+--------------+-----+------+------+\n",
      "|gender|ssc_p|  ssc_b|hsc_p|  hsc_b|   hsc_s|degree_p| degree_t|workex|etest_p|specialisation|mba_p|status|salary|\n",
      "+------+-----+-------+-----+-------+--------+--------+---------+------+-------+--------------+-----+------+------+\n",
      "|     M| 67.0| Others| 91.0| Others|Commerce|    58.0| Sci&Tech|    No|   55.0|        Mkt&HR| 58.8|Placed|270000|\n",
      "|     M|79.33|Central|78.33| Others| Science|   77.48| Sci&Tech|   Yes|   86.5|       Mkt&Fin|66.28|Placed|200000|\n",
      "|     M| 65.0|Central| 68.0|Central|    Arts|    64.0|Comm&Mgmt|    No|   75.0|       Mkt&Fin| 57.8|Placed|250000|\n",
      "|     M| 85.8|Central| 73.6|Central|Commerce|    73.3|Comm&Mgmt|    No|   96.8|       Mkt&Fin| 55.5|Placed|425000|\n",
      "|     M| 82.0|Central| 64.0|Central| Science|    66.0| Sci&Tech|   Yes|   67.0|       Mkt&Fin|62.14|Placed|252000|\n",
      "|     M| 73.0|Central| 79.0|Central|Commerce|    72.0|Comm&Mgmt|    No|  91.34|       Mkt&Fin|61.29|Placed|231000|\n",
      "|     M| 58.0|Central| 61.0|Central|Commerce|    60.0|Comm&Mgmt|   Yes|   62.0|        Mkt&HR|60.85|Placed|260000|\n",
      "|     M| 69.6|Central| 68.4|Central|Commerce|    78.3|Comm&Mgmt|   Yes|   60.0|       Mkt&Fin| 63.7|Placed|250000|\n",
      "|     F| 77.0|Central| 87.0|Central|Commerce|    59.0|Comm&Mgmt|    No|   68.0|       Mkt&Fin|68.63|Placed|218000|\n",
      "|     F| 65.0|Central| 75.0|Central|Commerce|    69.0|Comm&Mgmt|   Yes|   72.0|       Mkt&Fin|64.66|Placed|200000|\n",
      "|     M| 63.0|Central| 66.2|Central|Commerce|    65.6|Comm&Mgmt|   Yes|   60.0|       Mkt&Fin|62.54|Placed|300000|\n",
      "|     M| 60.0| Others| 67.0| Others|    Arts|    70.0|Comm&Mgmt|   Yes|  50.48|       Mkt&Fin|77.89|Placed|236000|\n",
      "|     M| 62.0| Others| 65.0| Others|Commerce|    66.0|Comm&Mgmt|    No|   50.0|        Mkt&HR| 56.7|Placed|265000|\n",
      "|     F| 79.0| Others| 76.0| Others|Commerce|    85.0|Comm&Mgmt|    No|   95.0|       Mkt&Fin|69.06|Placed|393000|\n",
      "|     F| 69.8| Others| 60.8| Others| Science|   72.23| Sci&Tech|    No|  55.53|        Mkt&HR|68.81|Placed|360000|\n",
      "|     F| 77.4| Others| 60.0| Others| Science|   64.74| Sci&Tech|   Yes|   92.0|       Mkt&Fin|63.62|Placed|300000|\n",
      "|     M| 76.5| Others| 97.7| Others| Science|   78.86| Sci&Tech|    No|   97.4|       Mkt&Fin|74.01|Placed|360000|\n",
      "|     M| 71.0| Others| 79.0| Others|Commerce|    66.0|Comm&Mgmt|   Yes|   94.0|       Mkt&Fin|57.55|Placed|240000|\n",
      "|     M| 63.0| Others| 67.0| Others|Commerce|    66.0|Comm&Mgmt|    No|   68.0|        Mkt&HR|57.69|Placed|265000|\n",
      "|     M|76.76| Others| 76.5| Others|Commerce|    67.5|Comm&Mgmt|   Yes|  73.35|       Mkt&Fin|64.15|Placed|350000|\n",
      "+------+-----+-------+-----+-------+--------+--------+---------+------+-------+--------------+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mba_place_clean = mba_place.drop(\"sl_no\").dropna()\n",
    "mba_place_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29776dc3-d6ce-44ef-a9ea-952dca42aab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+\n",
      "|Min_Salary|Max_Salary|       Avg_Salary|\n",
      "+----------+----------+-----------------+\n",
      "|    200000|    940000|288655.4054054054|\n",
      "+----------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## I.\tWhat’s the overall minimum, maximum and average salary from the dataset? ( 6 marks)  \n",
    "\n",
    "from pyspark.sql.functions import min, max, avg\n",
    "\n",
    "mba_place_clean.select(\n",
    "    min(\"salary\").alias(\"Min_Salary\"),\n",
    "    max(\"salary\").alias(\"Max_Salary\"),\n",
    "    avg(\"salary\").alias(\"Avg_Salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838938ec-cc8c-48a1-a42d-8520574853e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: 0"
     ]
    }
   ],
   "source": [
    "## II.\tHow many  female candidates are not placed ?  ( 4 marks)\n",
    "mba_place_clean.filter((mba_place_clean.gender == \"F\") & (mba_place_clean.status == \"Not Placed\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b681006-0a1a-4077-a98e-43ebb36219ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## III.\tOut of total male candidates placed, how many do not have any work experience ? (3 marks)\n",
    "mba_place_clean.filter(\n",
    "    (mba_place_clean.gender == \"M\") &\n",
    "    (mba_place_clean.status == \"Placed\") &\n",
    "    (mba_place_clean.workex == \"No\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6215d3bf-59f7-4101-af63-f3f1dbc2c587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Q1 - Section C - Oct 2024\n",
    "\n",
    "df_employee=spark.read.load(\"dbfs:/FileStore/tables/SparkSQL/employee_data_2000.csv\",\n",
    "                            format=\"csv\",\n",
    "                            sep=\",\",\n",
    "                            inferSchema=\"True\",\n",
    "                            header=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2873af6b-669f-4336-818e-c73b8ba2bf04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+---+----------+------+\n",
      "| ID|       name|gender|age|experience|salary|\n",
      "+---+-----------+------+---+----------+------+\n",
      "|  1| Employee_1|  Male| 31|        29| 85734|\n",
      "|  2| Employee_2|Female| 35|         3| 58621|\n",
      "|  3| Employee_3|  Male| 43|        12|146490|\n",
      "|  4| Employee_4|  Male| 38|        22|106441|\n",
      "|  5| Employee_5|  Male| 27|         7| 76121|\n",
      "|  6| Employee_6|Female| 50|        17|111418|\n",
      "|  7| Employee_7|  Male| 40|        11| 90197|\n",
      "|  8| Employee_8|  Male| 36|        30|114577|\n",
      "|  9| Employee_9|  Male| 42|        13|124634|\n",
      "| 10|Employee_10|Female| 56|        33|148034|\n",
      "| 11|Employee_11|  Male| 35|        17| 72431|\n",
      "| 12|Employee_12|  Male| 25|        33|147637|\n",
      "| 13|Employee_13|  Male| 27|        10|129245|\n",
      "| 14|Employee_14|  Male| 44|        11|130211|\n",
      "| 15|Employee_15|Female| 37|        14| 34837|\n",
      "| 16|Employee_16|  Male| 44|        30|141984|\n",
      "| 17|Employee_17|Female| 31|        28|120025|\n",
      "| 18|Employee_18|Female| 34|        25|113163|\n",
      "| 19|Employee_19|Female| 45|        17|101232|\n",
      "| 20|Employee_20|  Male| 45|        31| 78994|\n",
      "+---+-----------+------+---+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25beae3e-c904-4020-86e8-dbccb500e784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: 2000"
     ]
    }
   ],
   "source": [
    "df_employee.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d10f4d-a912-4659-bd45-0822d45a0d84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21238c5-de9b-4d9a-ae28-406c55b1fffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Average Salary|\n",
      "+--------------+\n",
      "|    89939.6625|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#What is the average salary of the employees in the dataset? ( 1 marks)\n",
    "from pyspark.sql.functions import avg\n",
    "df_employee.select(avg(df_employee.salary).alias(\"Average Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84d66879-0d6c-4a14-b781-65e0e578db42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|total numer of exp|\n",
      "+------------------+\n",
      "|             39076|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the total number of years of experience for all employees? (2 Marks)\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "df_employee.select(sum(df_employee.experience).alias(\"total numer of exp\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a963e704-a6d7-4fe3-bc1c-6ca6a8d1aa97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female|  984|\n",
      "|  Male| 1016|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the gender distribution in the DataFrame? ( 3 marks)\n",
    "# from pyspark.sql.funtions import count\n",
    "df_employee.groupBy(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c69149f2-4af9-4a18-82ff-bee51c235b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+\n",
      "|        name|experience|salary|\n",
      "+------------+----------+------+\n",
      "|Employee_110|        39|127557|\n",
      "|Employee_145|        39|142561|\n",
      "|Employee_176|        39|149873|\n",
      "|Employee_189|        39| 79593|\n",
      "|Employee_209|        39| 36089|\n",
      "|Employee_298|        39|147358|\n",
      "|Employee_359|        39| 57672|\n",
      "|Employee_381|        39| 37235|\n",
      "|Employee_397|        39| 75203|\n",
      "|Employee_448|        39| 44745|\n",
      "|Employee_479|        39| 34758|\n",
      "|Employee_490|        39| 35974|\n",
      "|Employee_497|        39| 51292|\n",
      "|Employee_657|        39|119385|\n",
      "|Employee_664|        39|125305|\n",
      "|Employee_697|        39|106646|\n",
      "|Employee_773|        39|111010|\n",
      "|Employee_805|        39|103717|\n",
      "|Employee_852|        39| 77766|\n",
      "|Employee_860|        39|121717|\n",
      "+------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the salary of the employee with the maximum experience? ( 3 marks)\n",
    "from pyspark.sql.functions import max\n",
    "max_exp = df_employee.select(max(df_employee.experience)).collect()[0][0]\n",
    "df_employee.filter(df_employee.experience==max_exp).select(df_employee.name,df_employee.experience,df_employee.salary).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42d7bfe-ee5b-4da6-9510-0f3143786512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: 1857"
     ]
    }
   ],
   "source": [
    "## How many employees are older than 22 years? (3 marks)\n",
    "\n",
    "df_employee.filter(df_employee.age>22).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2065e50c-eca1-4c83-b5c4-4d055b1ac112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the feature 'ID' and also remove null values from the DataFrame. (3 marks)\n",
    "df_employee_clean=df_employee.drop(df_employee.ID).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a747967-a2bc-404a-8547-1e87befb0690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a05153d-bf2f-48ab-924d-5fd032703199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Q paper - July 2024 \n",
    "univ_rank=spark.read.load(\"dbfs:/FileStore/tables/SparkSQL/qs_world_university_rankings_3000.csv\",\n",
    "                          format=\"csv\",\n",
    "                          sep=\",\",\n",
    "                          inferSchema=True,\n",
    "                          header=True)\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bdd9c8-d828-4847-8f4e-bf63bab21d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Institution Name: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Location Full: string (nullable = true)\n",
      " |-- Citations per Faculty: double (nullable = true)\n",
      " |-- International Students: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "univ_rank.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78b09b6-b9a8-4e3e-a761-e791469af451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: 3000"
     ]
    }
   ],
   "source": [
    "univ_rank.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8190e2c-16f0-4020-a11d-49e791c6f6c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+-------------------+---------------------+----------------------+\n",
      "|Institution Name|   Location|      Location Full|Citations per Faculty|International Students|\n",
      "+----------------+-----------+-------------------+---------------------+----------------------+\n",
      "| Institution_536|      Japan|     Delhi, Germany|                16.97|                    25|\n",
      "|  Institution_22|      India|       Paris, China|                21.24|                   100|\n",
      "| Institution_137|South Korea|       Tokyo, China|                79.57|                    94|\n",
      "| Institution_529|      China|New York, Australia|                34.27|                     0|\n",
      "|  Institution_55|     Canada|      London, Japan|                45.88|                    58|\n",
      "| Institution_188|      China|       Tokyo, China|                31.08|                    79|\n",
      "| Institution_564|      China|    Berlin, Germany|                95.47|                   100|\n",
      "| Institution_891|    Germany|       Delhi, China|                37.25|                    34|\n",
      "| Institution_606|         UK|    Beijing, France|                62.73|                   100|\n",
      "| Institution_768|        USA|        Toronto, UK|                33.25|                     7|\n",
      "| Institution_215|South Korea|      New York, USA|                27.26|                    41|\n",
      "| Institution_794|      Japan|   Toronto, Germany|                27.76|                   100|\n",
      "| Institution_933|        USA|       Tokyo, India|                90.19|                     3|\n",
      "| Institution_596|        USA|      London, Japan|                40.09|                    96|\n",
      "|  Institution_79|         UK|      Paris, France|                82.83|                    17|\n",
      "| Institution_562|        USA|    New York, Japan|                64.19|                   100|\n",
      "| Institution_758|    Germany|        Berlin, USA|                52.34|                     6|\n",
      "| Institution_846|        USA|      New York, USA|                32.32|                   100|\n",
      "| Institution_805|        USA|     Berlin, Canada|                82.06|                   100|\n",
      "|  Institution_24|South Korea|       Tokyo, India|                66.79|                   100|\n",
      "+----------------+-----------+-------------------+---------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "univ_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec8a655-8db4-4503-b198-8b520f76b599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: 951"
     ]
    }
   ],
   "source": [
    "# How many Institutions are included in the dataset? (2 mark)\n",
    "\n",
    "univ_rank.select(univ_rank[\"Institution Name\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09188f23-347e-45bf-96b5-72a6c666614b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: 283"
     ]
    }
   ],
   "source": [
    "# How many Institutions from ‘India' are included in dataset? (3 marks)\n",
    "\n",
    "univ_rank.filter(univ_rank[\"Location\"]=='India').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36db3718-9643-4f99-a2e7-fa2d92110245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: 246"
     ]
    }
   ],
   "source": [
    "univ_rank.filter(univ_rank[\"Location\"] == \"India\").select(\"Institution Name\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4af9cb-d767-4c6b-9f99-38d6485bb6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Institution Name|\n",
      "+----------------+\n",
      "| Institution_173|\n",
      "| Institution_279|\n",
      "|  Institution_22|\n",
      "| Institution_708|\n",
      "| Institution_751|\n",
      "| Institution_230|\n",
      "| Institution_101|\n",
      "| Institution_422|\n",
      "| Institution_782|\n",
      "| Institution_510|\n",
      "| Institution_274|\n",
      "| Institution_234|\n",
      "| Institution_428|\n",
      "| Institution_565|\n",
      "| Institution_255|\n",
      "| Institution_223|\n",
      "|  Institution_48|\n",
      "| Institution_916|\n",
      "| Institution_644|\n",
      "|  Institution_97|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "univ_rank.filter(univ_rank[\"Location\"] == \"India\").select(\"Institution Name\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3e4d4ad-3b88-4dba-a6a0-f700b36acfe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|avg(Citations per Faculty)|\n",
      "+--------------------------+\n",
      "|         55.22855123674909|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the average \"Citations per Faculty\" for universities located in 'India'? (5 marks)\n",
    "\n",
    "univ_rank.filter(univ_rank[\"Location\"]=='India').select(avg(\"Citations per Faculty\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc72ae60-0fb6-499c-ae3e-e00523eb0e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|avg_citations_per_faculty|\n",
      "+-------------------------+\n",
      "|        55.22855123674909|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "univ_rank.filter(univ_rank[\"Location\"] == \"India\") \\\n",
    "  .select(avg(\"Citations per Faculty\").alias(\"avg_citations_per_faculty\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779243bf-980f-47e3-9911-cb9bac1d1006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+-------------------+---------------------+----------------------+\n",
      "|Institution Name|   Location|      Location Full|Citations per Faculty|International Students|\n",
      "+----------------+-----------+-------------------+---------------------+----------------------+\n",
      "| Institution_536|      Japan|     Delhi, Germany|                16.97|                    25|\n",
      "|  Institution_22|      India|       Paris, China|                21.24|                   100|\n",
      "| Institution_137|South Korea|       Tokyo, China|                79.57|                    94|\n",
      "| Institution_529|      China|New York, Australia|                34.27|                     0|\n",
      "|  Institution_55|     Canada|      London, Japan|                45.88|                    58|\n",
      "| Institution_188|      China|       Tokyo, China|                31.08|                    79|\n",
      "| Institution_564|      China|    Berlin, Germany|                95.47|                   100|\n",
      "| Institution_891|    Germany|       Delhi, China|                37.25|                    34|\n",
      "| Institution_606|         UK|    Beijing, France|                62.73|                   100|\n",
      "| Institution_768|        USA|        Toronto, UK|                33.25|                     7|\n",
      "| Institution_215|South Korea|      New York, USA|                27.26|                    41|\n",
      "| Institution_794|      Japan|   Toronto, Germany|                27.76|                   100|\n",
      "| Institution_933|        USA|       Tokyo, India|                90.19|                     3|\n",
      "| Institution_596|        USA|      London, Japan|                40.09|                    96|\n",
      "|  Institution_79|         UK|      Paris, France|                82.83|                    17|\n",
      "| Institution_562|        USA|    New York, Japan|                64.19|                   100|\n",
      "| Institution_758|    Germany|        Berlin, USA|                52.34|                     6|\n",
      "| Institution_846|        USA|      New York, USA|                32.32|                   100|\n",
      "| Institution_805|        USA|     Berlin, Canada|                82.06|                   100|\n",
      "|  Institution_24|South Korea|       Tokyo, India|                66.79|                   100|\n",
      "+----------------+-----------+-------------------+---------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List Institutions where \"International Students\" percentage is 100 % along with their\n",
    "#location ( \"Location Full\"). (5 marks)\n",
    "\n",
    "univ_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca58390-9100-4657-b612-20b22a97c922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+\n",
      "|International Students|     Location Full|\n",
      "+----------------------+------------------+\n",
      "|                 100.0|      Paris, China|\n",
      "|                 100.0|   Berlin, Germany|\n",
      "|                 100.0|   Beijing, France|\n",
      "|                 100.0|  Toronto, Germany|\n",
      "|                 100.0|   New York, Japan|\n",
      "|                 100.0|     New York, USA|\n",
      "|                 100.0|    Berlin, Canada|\n",
      "|                 100.0|      Tokyo, India|\n",
      "|                 100.0|     Sydney, Japan|\n",
      "|                 100.0|     Berlin, India|\n",
      "|                 100.0|     Seoul, Canada|\n",
      "|                 100.0|    Tokyo, Germany|\n",
      "|                 100.0|     Delhi, France|\n",
      "|                 100.0|    Toronto, China|\n",
      "|                 100.0|    Beijing, Japan|\n",
      "|                 100.0|Paris, South Korea|\n",
      "|                 100.0|   New York, Japan|\n",
      "|                 100.0|   Toronto, France|\n",
      "|                 100.0|  Paris, Australia|\n",
      "|                 100.0|  New York, France|\n",
      "+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "univ_rank.filter(univ_rank[\"International Students\"]==100).select(\"International Students\",\"Location Full\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4790b26e-4721-4fbd-a1d0-99d7d9e3c024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "univ_rank = univ_rank.withColumn(\"International Students\", univ_rank[\"International Students\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8a8b0fc-87e8-48db-8c3f-b13facc1f2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Question no 2 - Section C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b74c7eb7-97b4-4f53-bebe-8fe987faf932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mba_placement = spark.read.load(\n",
    "    \"dbfs:/FileStore/tables/SparkSQL/mba_placement.csv\",\n",
    "    format=\"csv\",\n",
    "    sep=\",\",\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7a89eb-b228-4b92-8f98-f68f291d666d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- ssc_b: string (nullable = true)\n",
      " |-- hsc_p: double (nullable = true)\n",
      " |-- hsc_b: string (nullable = true)\n",
      " |-- hsc_s: string (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- degree_t: string (nullable = true)\n",
      " |-- workex: string (nullable = true)\n",
      " |-- etest_p: double (nullable = true)\n",
      " |-- specialisation: string (nullable = true)\n",
      " |-- mba_p: double (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mba_placement.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edeb0692-0b1a-4d15-8231-a752f0671d5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sl_no: integer (nullable = true)\n",
      " |-- ssc_p: double (nullable = true)\n",
      " |-- hsc_p: double (nullable = true)\n",
      " |-- degree_p: double (nullable = true)\n",
      " |-- etest_p: double (nullable = true)\n",
      " |-- mba_p: double (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- gender: double (nullable = false)\n",
      " |-- ssc_b: double (nullable = false)\n",
      " |-- hsc_b: double (nullable = false)\n",
      " |-- hsc_s: double (nullable = false)\n",
      " |-- degree_t: double (nullable = false)\n",
      " |-- workex: double (nullable = false)\n",
      " |-- specialisation: double (nullable = false)\n",
      " |-- status: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I.\tConvert all string columns into numeric values using StringIndexer transformer and make sure now DataFrame does not have any string columns anymore. (5 marks)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# List of string columns from schema\n",
    "string_cols = [\"gender\", \"ssc_b\", \"hsc_b\", \"hsc_s\", \"degree_t\", \"workex\", \"specialisation\", \"status\"]\n",
    "\n",
    "# Loop through each string column and apply StringIndexer\n",
    "for col_name in string_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_indexed\")\n",
    "    mba_placement = indexer.fit(mba_placement).transform(mba_placement).drop(col_name)  # drop original string column\n",
    "\n",
    "# Rename new indexed columns to original names\n",
    "for col_name in string_cols:\n",
    "    mba_placement = mba_placement.withColumnRenamed(col_name + \"_indexed\", col_name)\n",
    "\n",
    "# Confirm schema no longer contains string columns\n",
    "mba_placement.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5658a7-66d6-431f-b2fd-734d8053a0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+------------------------------------------------------------------+------+\n",
      "|features                                                          |salary|\n",
      "+------------------------------------------------------------------+------+\n",
      "|[1.0,67.0,91.0,58.0,55.0,58.8,0.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0]    |270000|\n",
      "|[2.0,79.33,78.33,77.48,86.5,66.28,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0]|200000|\n",
      "|(14,[0,1,2,3,4,5,8,9],[3.0,65.0,68.0,64.0,75.0,57.8,1.0,2.0])     |250000|\n",
      "|[4.0,56.0,52.0,52.0,66.0,59.43,0.0,0.0,1.0,1.0,1.0,0.0,1.0,1.0]   |null  |\n",
      "|(14,[0,1,2,3,4,5,8],[5.0,85.8,73.6,73.3,96.8,55.5,1.0])           |425000|\n",
      "+------------------------------------------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using vectorAssembler combines all columns (except target column i.e., 'salary') of spark DataFrame into a single column (name as features). Make sure DataFrame now contains only two columns: features and salary. ( 5 marks)\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List all columns except 'salary' (target column)\n",
    "feature_cols = [col for col in mba_placement.columns if col != 'salary']\n",
    "\n",
    "# Initialize VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# Transform the DataFrame\n",
    "df_vectorized = assembler.transform(mba_placement)\n",
    "\n",
    "# Select only the 'features' and 'salary' columns\n",
    "final_df = df_vectorized.select('features', 'salary')\n",
    "\n",
    "# Show resulting schema\n",
    "final_df.printSchema()\n",
    "final_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd291f2-f7a7-479b-a19b-05bfd0b2d8de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with null values: 67\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count rows with null in 'features' or 'salary'\n",
    "null_counts = final_df.filter(\n",
    "    col(\"features\").isNull() | col(\"salary\").isNull()\n",
    ").count()\n",
    "\n",
    "print(f\"Number of rows with null values: {null_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b446873-ac35-4271-b02f-7f674940a1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|salary|\n",
      "+--------------------+------+\n",
      "|[1.0,67.0,91.0,58...|270000|\n",
      "|[2.0,79.33,78.33,...|200000|\n",
      "|(14,[0,1,2,3,4,5,...|250000|\n",
      "|(14,[0,1,2,3,4,5,...|425000|\n",
      "|[8.0,82.0,64.0,66...|252000|\n",
      "|(14,[0,1,2,3,4,5,...|231000|\n",
      "|[11.0,58.0,61.0,6...|260000|\n",
      "|(14,[0,1,2,3,4,5,...|250000|\n",
      "|(14,[0,1,2,3,4,5,...|218000|\n",
      "|[16.0,65.0,75.0,6...|200000|\n",
      "|(14,[0,1,2,3,4,5,...|300000|\n",
      "|[20.0,60.0,67.0,7...|236000|\n",
      "|(14,[0,1,2,3,4,5,...|265000|\n",
      "|(14,[0,1,2,3,4,5,...|393000|\n",
      "|[23.0,69.8,60.8,7...|360000|\n",
      "|[24.0,77.4,60.0,6...|300000|\n",
      "|[25.0,76.5,97.7,7...|360000|\n",
      "|(14,[0,1,2,3,4,5,...|240000|\n",
      "|(14,[0,1,2,3,4,5,...|265000|\n",
      "|(14,[0,1,2,3,4,5,...|350000|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop rows where any column has a null value\n",
    "clean_df = final_df.dropna()\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "clean_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2579cb42-d0ba-4b03-9266-5e87c38a7a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with null values: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count rows with null in 'features' or 'salary'\n",
    "null_counts = clean_df.filter(\n",
    "    col(\"features\").isNull() | col(\"salary\").isNull()\n",
    ").count()\n",
    "\n",
    "print(f\"Number of rows with null values: {null_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720c1f60-9604-4d8d-8877-a8d384013045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Count: 114\n",
      "Test Set Count: 34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#III.\tSplit the vectorized dataframe into training and test sets with one fourth records being held for testing (3marks)\n",
    "# Split the DataFrame: 75% for training, 25% for testing\n",
    "train_df, test_df = clean_df.randomSplit([0.75, 0.25], seed=42)\n",
    "\n",
    "# Optional: Check counts\n",
    "print(\"Training Set Count:\", train_df.count())\n",
    "print(\"Test Set Count:\", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c9bca1-8ba5-4175-bd02-517b3eee68e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [65.57726801868472,-673.6507109523955,767.0893998286092,-657.2210143693152,-229.45851076566956,3840.740918203088,-36955.001336838664,1651.7421763093039,-7644.5914114114275,-20483.91488604545,54043.63654414776,6768.795504260067,-13105.867138402395,0.0]\n",
      "Intercept: 105443.38798964706\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#IV.\tBuild a LinearRegression model on train set  use featuresCol=\"features\" and  'salary'(6 marks)\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"salary\")\n",
    "\n",
    "# Fit the model on training data\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Print model coefficients and intercept\n",
    "print(\"Coefficients:\", lr_model.coefficients)\n",
    "print(\"Intercept:\", lr_model.intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1607fb1c-cdf5-42a8-b50a-1a787bbb0a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data: 16779369846.712732\n"
     ]
    }
   ],
   "source": [
    "# V.\tPerform prediction on the testing data and Print MSE value? ( 6 marks)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Step 1: Predict salaries on the test set\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Step 2: Initialize evaluator for MSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"salary\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mse\"\n",
    ")\n",
    "\n",
    "# Step 3: Compute Mean Squared Error\n",
    "mse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcd64d3e-ba93-45a6-90cd-1813acedb4e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Oct 2024 Section C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f61e7b5-40fd-40f9-8120-b608134d8c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I.\tConvert all string columns into numeric values using StringIndexer transformer and make sure now DataFrame does not have any string columns anymore. (5 marks) \n",
    "\n",
    "emp_data = spark.read.load(\n",
    "    \"dbfs:/FileStore/tables/SparkSQL/employee_data_2000.csv\",\n",
    "    format=\"csv\",\n",
    "    sep=\",\",\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5263b7e-a7f9-4e26-aa52-e33fcfaf7320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950145b8-4829-48e9-abc7-38ff42e8d1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- name: double (nullable = false)\n",
      " |-- gender: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I.\tConvert all string columns into numeric values using StringIndexer transformer and make sure now DataFrame does not have any string columns anymore. (5 marks) \n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# List of string columns from schema\n",
    "string_cols = [\"name\", \"gender\"]\n",
    "\n",
    "# Loop through each string column and apply StringIndexer\n",
    "for col_name in string_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_indexed\")\n",
    "    emp_data = indexer.fit(emp_data).transform(emp_data).drop(col_name)  # drop original string column\n",
    "\n",
    "# Rename new indexed columns to original names\n",
    "for col_name in string_cols:\n",
    "    emp_data = emp_data.withColumnRenamed(col_name + \"_indexed\", col_name)\n",
    "\n",
    "# Confirm schema no longer contains string columns\n",
    "emp_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc14e813-a1d2-4533-b08c-af7379af6b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_data=emp_data.drop(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae66c24-7977-4336-9ba4-16959fe74ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- name: double (nullable = false)\n",
      " |-- gender: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0735af9b-d532-4a0c-8b3f-1d5d8bf167cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+------+------+\n",
      "|age|experience|salary|  name|gender|\n",
      "+---+----------+------+------+------+\n",
      "| 31|        29| 85734|   0.0|   0.0|\n",
      "| 35|         3| 58621|1111.0|   1.0|\n",
      "| 43|        12|146490|1223.0|   0.0|\n",
      "| 38|        22|106441|1334.0|   0.0|\n",
      "| 27|         7| 76121|1445.0|   0.0|\n",
      "| 50|        17|111418|1556.0|   1.0|\n",
      "| 40|        11| 90197|1667.0|   0.0|\n",
      "| 36|        30|114577|1778.0|   0.0|\n",
      "| 42|        13|124634|1889.0|   0.0|\n",
      "| 56|        33|148034|   1.0|   1.0|\n",
      "+---+----------+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e9c4b3-a7c5-4724-9041-5837c4be32d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+----------------------+------+\n",
      "|features              |salary|\n",
      "+----------------------+------+\n",
      "|[31.0,29.0,0.0,0.0]   |85734 |\n",
      "|[35.0,3.0,1111.0,1.0] |58621 |\n",
      "|[43.0,12.0,1223.0,0.0]|146490|\n",
      "|[38.0,22.0,1334.0,0.0]|106441|\n",
      "|[27.0,7.0,1445.0,0.0] |76121 |\n",
      "+----------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#II.\tUsing vectorAssembler combines all columns (except target column i.e., 'salary') of spark DataFrame into a single column (name as features). Make sure DataFrame now contains only two columns: features and salary. (6 marks)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List all columns except 'salary' (target column)\n",
    "feature_cols = [col for col in emp_data.columns if col != 'salary']\n",
    "\n",
    "# Initialize VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# Transform the DataFrame\n",
    "df_vectorized = assembler.transform(emp_data)\n",
    "\n",
    "# Select only the 'features' and 'salary' columns\n",
    "final_df = df_vectorized.select('features', 'salary')\n",
    "\n",
    "# Show resulting schema\n",
    "final_df.printSchema()\n",
    "final_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98430594-9fee-4ca1-8268-2fa478aaa694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with null values: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count rows with null in 'features' or 'salary'\n",
    "null_counts = final_df.filter(\n",
    "    col(\"features\").isNull() | col(\"salary\").isNull()\n",
    ").count()\n",
    "\n",
    "print(f\"Number of rows with null values: {null_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8588768-5149-4e0b-a206-7172285fe91c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "scaler_model = scaler.fit(final_df)\n",
    "df_scaled = scaler_model.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7081d9-cff8-4f87-8ed9-4cf1d172d4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Count: 1540\n",
      "Test Set Count: 460\n"
     ]
    }
   ],
   "source": [
    "#III.\tSplit the vectorized dataframe into training and test sets with one fourth records being held for testing (4 marks)\n",
    "\n",
    "train_df, test_df = df_scaled.randomSplit([0.75, 0.25], seed=42)\n",
    "\n",
    "# Optional: Check counts\n",
    "print(\"Training Set Count:\", train_df.count())\n",
    "print(\"Test Set Count:\", test_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e498a7-115e-462d-b690-ef889a4f0749",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [20.324682791649195,-18.513061974813336,0.011092798156402553,-4064.8556227011736]\n",
      "Intercept: 91082.09974939919\n"
     ]
    }
   ],
   "source": [
    "#IV.\tBuild a LinerRegression model on train set use featuresCol=\"features\" and 'salary'(5 marks)\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Define the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"salary\")\n",
    "\n",
    "# Fit the model on training data\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Print model coefficients and intercept\n",
    "print(\"Coefficients:\", lr_model.coefficients)\n",
    "print(\"Intercept:\", lr_model.intercept)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942f947f-2eb7-4c5f-82ae-35591b5589ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data: 1223858469.1866179\n"
     ]
    }
   ],
   "source": [
    "# V.\tV.\tPerform prediction on the testing data and Print RMSE,MAE value? (5 marks)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Step 1: Predict salaries on the test set\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Step 2: Initialize evaluator for MSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"salary\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"mse\"\n",
    ")\n",
    "\n",
    "# Step 3: Compute Mean Squared Error\n",
    "mse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE) on test data: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddb5dd17-e83f-4ca2-ab7c-aca697a6c792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# july 2024 Paper review \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0bd8080-abf5-49d1-8645-d564cb3936b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Recreate the Dataframe by Dropping all the rows where 'QS Overall Score' is\n",
    "# mention as '-' and also convert it as float type. (2 marks )\n",
    "\n",
    "univ_data = spark.read.load(\n",
    "    \"dbfs:/FileStore/tables/SparkSQL/QS_World_University_Rankings_1200.csv\",\n",
    "    format=\"csv\",\n",
    "    sep=\",\",\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319a3f07-74f7-4a0b-acca-6de3f8ac977c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Institution Name: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Location Full: string (nullable = true)\n",
      " |-- Citations per Faculty: double (nullable = true)\n",
      " |-- International Students: integer (nullable = true)\n",
      " |-- QS Overall Score: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "univ_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1113440-bb7f-4624-8303-5ec38fb729be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[80]: 1200"
     ]
    }
   ],
   "source": [
    "univ_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4627f054-2cc6-496f-9d4e-962c499283f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Institution Name: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Location Full: string (nullable = true)\n",
      " |-- Citations per Faculty: double (nullable = true)\n",
      " |-- International Students: integer (nullable = true)\n",
      " |-- QS Overall Score: float (nullable = true)\n",
      "\n",
      "+----------------+--------+---------------+---------------------+----------------------+----------------+\n",
      "|Institution Name|Location|  Location Full|Citations per Faculty|International Students|QS Overall Score|\n",
      "+----------------+--------+---------------+---------------------+----------------------+----------------+\n",
      "|    University_1|   India|   Delhi, India|                82.62|                    33|           48.03|\n",
      "|    University_2|      UK|     London, UK|                 75.1|                    49|           63.81|\n",
      "|    University_3|     USA|Toronto, Canada|                45.64|                    83|           79.76|\n",
      "|    University_4|     USA|   Delhi, India|                83.48|                    62|           85.43|\n",
      "|    University_5|     USA|   Delhi, India|                77.12|                    73|           66.77|\n",
      "+----------------+--------+---------------+---------------------+----------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recreate the Dataframe by Dropping all the rows where 'QS Overall Score' is\n",
    "# mention as '-' and also convert it as float type. (2 marks )\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Step 1: Filter out rows where 'QS Overall Score' is '-'\n",
    "df_cleaned = univ_data.filter(col(\"QS Overall Score\") != \"-\")\n",
    "\n",
    "# Step 2: Cast 'QS Overall Score' to float\n",
    "df_cleaned = df_cleaned.withColumn(\"QS Overall Score\", col(\"QS Overall Score\").cast(\"float\"))\n",
    "\n",
    "# Step 3: Verify schema and sample data\n",
    "df_cleaned.printSchema()\n",
    "df_cleaned.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866897f0-11a1-4d49-9ed9-da797910bb74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: 582"
     ]
    }
   ],
   "source": [
    "df_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c78915-d609-4315-8c31-243ed3a8cba3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Remove all the rows with any missing entry.( 3 marks)\n",
    "\n",
    "df_no_nulls = df_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bae0b66-69ec-426a-997b-c637e25ea316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[83]: 582"
     ]
    }
   ],
   "source": [
    "df_no_nulls.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2efae22-5613-48b7-9879-71afd184b7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Institution Name: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Location Full: string (nullable = true)\n",
      " |-- Citations per Faculty: double (nullable = true)\n",
      " |-- International Students: integer (nullable = true)\n",
      " |-- QS Overall Score: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_no_nulls.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9feaf074-fc2e-45f8-ae75-50705d3ee45c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Citations per Faculty: double (nullable = true)\n",
      " |-- International Students: integer (nullable = true)\n",
      " |-- QS Overall Score: float (nullable = true)\n",
      " |-- Institution Name: double (nullable = false)\n",
      " |-- Location: double (nullable = false)\n",
      " |-- Location Full: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Step 1: Identify string columns\n",
    "string_cols = [field.name for field in df_no_nulls.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "# Step 2: Apply StringIndexer to all string columns and drop originals\n",
    "for col_name in string_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_indexed\")\n",
    "    df_no_nulls = indexer.fit(df_no_nulls).transform(df_no_nulls).drop(col_name)\n",
    "\n",
    "# Step 3: Rename indexed columns back to original names\n",
    "for col_name in string_cols:\n",
    "    df_no_nulls = df_no_nulls.withColumnRenamed(col_name + \"_indexed\", col_name)\n",
    "\n",
    "# Final DataFrame\n",
    "df_indexed = df_no_nulls\n",
    "\n",
    "# Step 4: Confirm schema has no string columns\n",
    "df_indexed.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e18d2d-f08a-446f-a966-4251558def48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Using vectorAssembler combines all columns, except target column i.e. 'QS Overall\n",
    "Score', of spark DataFrame into single column (name it as features). Make sure\n",
    "DataFrame now contains only two columns, 'features' and 'QS Overall Score'. (5\n",
    "marks)'''\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Exclude the target column\n",
    "feature_cols = [col for col in df_indexed.columns if col != \"QS Overall Score\"]\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_us\")\n",
    "df_vector = assembler.transform(df_indexed).select(\"features_us\", \"QS Overall Score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f5ecc6-ae87-4b58-9d5f-7e479bcca079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features_us\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_vector)\n",
    "df_scaled = scaler_model.transform(df_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4147aa24-b7c2-472c-8e9f-0779e1048ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Split the vectorised Dataframe into training and test sets with one fifth records being\n",
    "#held for testing. (2 marks)\n",
    "\n",
    "train_df, test_df = df_scaled.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c44cb32b-05f0-472c-a058-1e2aa9fc32b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Train default LinearRegression model with features as 'featuresCol' and ‘QS Overall\n",
    "Score’ as label on training set. (3 marks)'''\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"QS Overall Score\")\n",
    "lr_model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afce48ea-15c3-40e6-bbf1-16fbb84c826e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data: 17.49442054892468\n"
     ]
    }
   ],
   "source": [
    "'''Perform prediction on the testing data and Print RMSE value. (5 marks)'''\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Perform prediction\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"QS Overall Score\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e406fb7-9e65-4919-8f54-051364943657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Section B - Q1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f81bafa7-a094-4c4c-8e7e-7472855882e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write HDFS shell commands for the following- \n",
    "1.\t Print contents of the directory  by path, showing the names, permissions, owner, size and modification date for each entry? (2 marks)\n",
    "2.\tHow do you upload multiple files from the local system to a directory in HDFS?. (2 marks)\n",
    "3.\tHow do you display the contents of a file in HDFS line by line (paged view)? (2 marks)\n",
    "4.\tWrite command to remove a file or directory identified by path in HDFS and recursively delete any child entries. (2 marks)\n",
    "5.\tWrite command to copy the ‘testfile’ of the hadoop filesystem to the local file system  (2marks)\n",
    "\n",
    "Note: Consider InputDir, OutputDir, XYZ, SampleDir, and file.txt are under the present working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97413b9-a0bb-467f-9c89-ae02f88b9e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1> 1. Print contents of the directory by path, showing the names, permissions, owner, size and modification date for each entry? \n",
    "\n",
    " hdfs dfs -ls /user/hadoop/InputDir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b8b49c8-a777-4c1e-a1ee-8e00f4e6d861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "2>2. How do you upload multiple files from the local system to a directory in HDFS?. (2 marks) \n",
    "\n",
    "hdfs dfs -put /test/*.txt /user/hadoop/SampleDir\n",
    "hdfs dfs -put file1.txt file2.txt file3.txt /path/in/hdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd300e3-d0b1-45a3-b291-abc455a702c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "3. How do you display the contents of a file in HDFS line by line (paged view)?\n",
    "\n",
    "hdfs dfs -cat /user/hadoop/file.txt | less\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d1967eb-a45c-479d-ac40-40f0ed8bf105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "4. Write command to remove a file or directory identified by path in HDFS and recursively delete any child entries. (2 marks)\n",
    "\n",
    "hdfs dfs -rm -r /user/hadoop/XYZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baf6e4d5-35e8-4507-9069-139e6ac7d3ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "5. Write command to copy the ‘testfile’ of the hadoop filesystem to the local file system (2marks)\n",
    "\n",
    "hdfs dfs -get /user/hadoop/testfile ./testfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89fad246-4f24-4615-9e25-acc557514df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write HDFS shell commands for the following-\n",
    "1. To Copy file1.txt from folder InputDir to OutputDir as file2.txt. (2 marks)\n",
    "2. To Delete an empty directory named as XYZ. (2 marks)\n",
    "3. To List the files and directories under folder named SampleDir. (2 marks)\n",
    "4. To Recursively list the files and directories exist under folder named SampleDir. (2 marks)\n",
    "5. To change the Permission of file named file.txt to Read only (444) (2marks)\n",
    "Note: Consider InputDir, OutputDir, XYZ, SampleDir, and file.txt are under the present\n",
    "working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56967e05-2d5c-46b6-ad75-97a3c2d315b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "To Copy file1.txt from folder InputDir to OutputDir as file2.txt. (2 marks)\n",
    "\n",
    "hdfs dfs -cp /user/hadoop/InputDir/file1.txt /user/hadoop/OutputDir/file2.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "749cde8d-c509-46b8-a244-b2d293d82d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "To Delete an empty directory named as XYZ. (2 marks)\n",
    "\n",
    "hdfs dfs -rmdir /user/hadoop/XYZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e16f05-88e9-4cb3-b4d0-fda9b099938e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "To List the files and directories under folder named SampleDir. (2 marks)\n",
    "\n",
    "hdfs dfs -ls /user/hadoop/SampleDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11eff447-d52c-4d16-9a48-a94d53c062da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "To Recursively list the files and directories exist under folder named SampleDir. (2 marks)\n",
    "\n",
    "hdfs dfs -ls -R /user/hadoop/SampleDir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea962285-0d69-435b-9e3d-97017c593677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Change the permission of file named file.txt to read-only (444)\n",
    "\n",
    "hdfs dfs -chmod 444 /user/hadoop/file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c2bf253-56aa-4441-a574-0f3327963339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2022 paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df16e97c-e9ea-433d-9fae-57d270b0d1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "To print the version of installed Hadoop (2 marks)\n",
    "\n",
    "hadoop version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df4e983-c1aa-48ef-916b-2216863de89b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "To copy file1.txt from folder InputDir to OutputDir as file2.txt (3 marks)\n",
    "\n",
    "hdfs dfs -cp /user/hadoop/InputDir/file1.txt /user/hadoop/OutputDir/file2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12ddea4d-f9ef-4bd5-aa5e-69d99f3c1265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " To delete an empty directory named XYZ (3 marks)\n",
    "\n",
    " hdfs dfs -rmdir /user/hadoop/XYZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f536521-12e3-4bca-b218-7a7960ca2406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "To list the contents of a folder named SampleDir (3 mark\n",
    "                                                  \n",
    " hdfs dfs -ls /user/hadoop/SampleDir                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5e65837-4bab-4b2b-b136-f459096b5599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "To fetch the usage instructions/details of the mkdir command (3 marks)\n",
    "\n",
    "hdfs dfs -help mkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b814f6e4-bd4c-43f7-bd2a-890c73564391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Section B - Question B Rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "476d682e-925e-4eaa-831c-95c0f1666405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " Considering sc as spark content object, and rdd as RDD object, write Spark commands to,\n",
    "1.\tCreate an RDD from the following list: List(1, 2, 3, 4, 5,6,7,8,9,10). (2 marks)\n",
    "2.\tDisplay/Print first four elements of the RDD. (2 marks)\n",
    "3.\tDisplay/Print the first element of the RDD. (2 marks)\n",
    "4.\tExplain with example map,filter Apache Spark transformations.(4 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf5973e8-b9da-44a5-8e6a-2d6703fc312e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.\tCreate an RDD from the following list: List(1, 2, 3, 4, 5,6,7,8,9,10). (2 marks)\n",
    "\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b12b413-3597-4557-b64b-c4918fa816c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: [1, 2, 3, 4]"
     ]
    }
   ],
   "source": [
    "# Display/Print first four elements of the RDD. (2 marks)\n",
    "\n",
    "print(rdd.take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6efd68e6-e48c-45af-9217-2d32148d087a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#3.\tDisplay/Print the first element of the RDD. (2 marks)\n",
    "\n",
    "print(rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f247388b-543d-4fbb-a8c3-0fcb87d08db2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    }
   ],
   "source": [
    "# Explain with example map,filter Apache Spark transformations.(4 marks)\n",
    "\n",
    "# Definition: map is used to transform each element of the RDD.\n",
    "\n",
    "rdd_mapped = rdd.map(lambda x: x * 2)\n",
    "print(rdd_mapped.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24cfe52b-abd6-49a5-b892-f2d33ee0c883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "# Definition: filter is used to retain only those elements that satisfy a condition.\n",
    "\n",
    "\n",
    "rdd_filtered = rdd.filter(lambda x: x % 3 == 0)\n",
    "print(rdd_filtered.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d3dca28-694e-4fae-91e7-af1de3410708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Considering sc as spark content object, and rdd as RDD object, write Spark commands to,\n",
    "1.\tCreate an RDD from the following list: List(1, 2, 3, 4, 5,6). (2 marks)\n",
    "2.\tDisplay/Print first four elements of the RDD. (2 marks)\n",
    "3.\tDisplay/Print the first element of the RDD. (2 marks)\n",
    "4.\tDisplay/Print the number of elements in the RDD. (2 marks)\n",
    "5.\tDisplay sum of all elements of the RDD. (2 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a043ce-6a33-465f-9b52-9f498844495e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.\tCreate an RDD from the following list: List(1, 2, 3, 4, 5,6). (2 marks)\n",
    "rdd=sc.parallelize([1, 2, 3, 4, 5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98a0017-dea3-445b-a113-9953ac227871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#2.\tDisplay/Print first four elements of the RDD. (2 marks)\n",
    "\n",
    "print(rdd.take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290bf87d-ad95-4944-b387-1d422a879f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# 3.\tDisplay/Print the first element of the RDD. (2 marks)\n",
    "\n",
    "print(rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e551987-e9fa-4fed-8565-bfac634a1a64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# Display/Print the number of elements in the RDD. (2 marks)\n",
    "\n",
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf53fe4d-1856-471f-a9b6-c9c7a51ee73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# Display sum of all elements of the RDD. (2 marks)\n",
    "print(rdd.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0798dcf4-b7ac-48a6-a571-9678ed8f83c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Considering sc as spark content object, and rdd as RDD object, write Spark commands to,\n",
    "1. Create an RDD from the following list: List(1, 2, 3, 4, 5,6). (2 marks)\n",
    "2. Read/load a text file located at \"/path/to/file.txt\" into an RDD. (2 marks)\n",
    "3. Filter out the even numbers from RDD. (2 marks)\n",
    "4. Map each element in the RDD to its square. (2 marks)\n",
    "5. Count the number of elements in the RDD. (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2eeedb-b482-4b9a-8cdd-b20ec035c1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# . Create an RDD from the following list: List(1, 2, 3, 4, 5,6). (2 marks)\n",
    "rdd=sc.parallelize([1, 2, 3, 4, 5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e323dbf-cdac-47ef-8b23-e1626def9f65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Read/load a text file located at \"/path/to/file.txt\" into an RDD. (2 marks)\n",
    "text_rdd = sc.textFile(\"/path/to/file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72683dfc-fd45-4463-8c8c-056c35a53af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "# 3. Filter out the even numbers from RDD. (2 marks)\n",
    "\n",
    "odd_rdd = rdd.filter(lambda x: x % 2 != 0)\n",
    "print(odd_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad0e1a5-f94c-4032-b50a-17716173a87b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36]\n"
     ]
    }
   ],
   "source": [
    "# 4. Map each element in the RDD to its square. (2 marks)\n",
    "sq_rdd=rdd.map(lambda x:x**2)\n",
    "print(sq_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61ee809-af9c-426e-807f-2016925ef319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# 5. Count the number of elements in the RDD. (2 marks)\n",
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d21aeaed-0510-4477-b463-593cdecefd0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Write below queries in Hive; \n",
    "1.\tWrite hive query to create databases name: emp. (2 Marks) \n",
    "2.\tWrite hive query to CREATE EXTERNAL TABLE in emp name it- employee with emp_id,name,location, dep,designation and salary as columns (4 Marks) \n",
    "3.\tWrite a hive query to perform an inner join on the Table1 and Table 2 on ‘id’ column (4 Marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7109017-8a44-491b-92f9-d44579c60616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write hive query to create databases name: emp. (2 Marks) \n",
    "\n",
    "%sql\n",
    "CREATE DATABASE emp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fbb623f-416c-46e0-936e-09f82a2f912f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write hive query to CREATE EXTERNAL TABLE in emp name it- employee with emp_id,name,location, dep,designation and salary as columns (4 Marks) \n",
    "%sql\n",
    "CREATE EXTERNAL TABLE employee (\n",
    "    emp_id INT,\n",
    "    name STRING,\n",
    "    location STRING,\n",
    "    dep STRING,\n",
    "    designation STRING,\n",
    "    salary FLOAT\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY '\\t' \n",
    "LINES TERMINATED BY '\\n'\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/user/hive/warehouse/employee/'\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05bf34e6-dc13-4889-be79-af2bf6c8844c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write a hive query to perform an inner join on the Table1 and Table 2 on ‘id’ column (4 Marks) \n",
    "%sql\n",
    "SELECT *\n",
    "FROM Table1 t1\n",
    "JOIN Table2 t2\n",
    "ON t1.id = t2.id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19348ac2-383d-41e1-81c1-09525354fca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Write below queries in Hive; \n",
    "1.\tWrite hive query to create databases name: emp. (2 Marks) \n",
    "2.\tWrite hive query to CREATE EXTERNAL TABLE in emp name it- employee with emp_id,name,location, dep,designation and salary as columns (4 Marks) \n",
    "3.\tWrite a hive query to print the average salary of employees based on location. (4 Marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aa2d04b-ed52-4054-9656-325325f85f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.\tWrite hive query to create databases name: emp. (2 Marks) \n",
    "%sql\n",
    "CREATE DATABASE emp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50d1b042-3505-4c15-8004-cc59b54c54c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.\tWrite hive query to CREATE EXTERNAL TABLE in emp name it- employee with emp_id,name,location, dep,designation and salary as columns (4 Marks)\n",
    "%sql\n",
    "CREATE EXTERNAL TABLE emp.employee (\n",
    "    emp_id INT,\n",
    "    name STRING,\n",
    "    location STRING,\n",
    "    dep STRING,\n",
    "    designation STRING,\n",
    "    salary FLOAT\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY '\\t' \n",
    "LINES TERMINATED BY '\\n'\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f5ddc4a-5ab7-40c4-8b4c-562bd99873f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3.\tWrite a hive query to print the average salary of employees based on location. (4 Marks) \n",
    "%sql\n",
    "SELECT location, AVG(salary) AS average_salary\n",
    "FROM emp.employee\n",
    "GROUP BY location;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a31ad0c-6e26-4c7d-a202-6d34a937f517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1. Write hive query to create databases name: anotherDB. (2 Marks)\n",
    "2. Write hive query to CREATE EXTERNAL TABLE in anotherDB name it- orders1\n",
    "with order_id, order_date, order_customer_id and order_status as columns(4\n",
    "Marks)\n",
    "3. Write hive query to load data in orders1 table using file which is available in local file\n",
    "system. (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05ba0d1e-34e2-4e5b-a2a1-6567fe7e46a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#1. Write hive query to create databases name: anotherDB. (2 Marks)\n",
    "%sql\n",
    "CREATE DATABASE anotherDB;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb5597b4-5163-49c3-a91b-28abebbd1808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Write hive query to CREATE EXTERNAL TABLE in anotherDB name it- orders1\n",
    "#with order_id, order_date, order_customer_id and order_status as columns(4\n",
    "#Marks)\n",
    "\n",
    "CREATE EXTERNAL TABLE anotherDB.orders1 (\n",
    "    order_id INT,\n",
    "    order_date STRING,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED \n",
    "FIELDS TERMINATED BY ',' \n",
    "LINES TERMINATED BY '\\n'\n",
    "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "064acfe3-580a-4fce-8c9b-333185644a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Write hive query to load data in orders1 table using file which is available in local file\n",
    "#system. (4 Marks)\n",
    "\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/path/to/your/file.csv' \n",
    "INTO TABLE anotherDB.orders1;\n",
    "\n",
    "# if we need to load data from HDFS \n",
    "\n",
    "LOAD DATA INPATH '/path/in/hdfs/orders1.csv' \n",
    "INTO TABLE anotherDB.orders1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53dd98f5-0523-4e59-89cb-770e64d1ff34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Section B - Question d \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27bcb3db-bd51-451c-8132-463f7679e782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Write commands / query in MongoDB\n",
    "1. Create a collection named ‘product collection’. (2 mark)\n",
    "2. Insert 5 documents in product collection based on name, rating, brand.(2 mark)\n",
    "3. Write query to find those products which have received 5/5 rating.(3 mark)\n",
    "4. Write a query to update those records where the product name is AC to \"Air conditioner\"and print it. (3 mark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb85f60-1e30-4069-803d-2e7a84158ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Create a collection named ‘product collection’. (2 mark)\n",
    "\n",
    "db.createCollection(\"product_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f350e88-1f08-4857-b31a-20e18e828597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#. Insert 5 documents in product collection based on name, rating, brand.(2 mark)\n",
    "db.product_collection.insertMany([\n",
    "  { name: \"AC\", rating: 5, brand: \"Samsung\" },\n",
    "  { name: \"TV\", rating: 4, brand: \"Sony\" },\n",
    "  { name: \"Washing Machine\", rating: 5, brand: \"LG\" },\n",
    "  { name: \"Microwave\", rating: 3, brand: \"Panasonic\" },\n",
    "  { name: \"Refrigerator\", rating: 5, brand: \"Whirlpool\" }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9ed46e4-946f-42d9-a136-f32961192fa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Write query to find those products which have received 5/5 rating.(3 mark)\n",
    "\n",
    "db.product_collection.find({ rating: 5 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6958790-3edc-44bf-b4c4-ecd5a86cb70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Write a query to update those records where the product name is AC to \"Air conditioner\"and print it. (3 mark)\n",
    "\n",
    "\n",
    "db.product_collection.updateMany(\n",
    "  { name: \"AC\" },\n",
    "  { $set: { name: \"Air conditioner\" } }\n",
    ")\n",
    "\n",
    "# Print updated documents\n",
    "db.product_collection.find({ name: \"Air conditioner\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156c7924-35c8-4223-83e3-86d1515acbdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Write commands / query in MongoDB\n",
    "1. Create a collection named users. (2 mark)\n",
    "2. Insert below records in users. (2 marks)\n",
    "{ name:Abhishek},{name: Raja, age: 24},{name: Ravi,age: 34},{name: Ram, age: 45},{name: Roopa, age: 44},{name: Tina, age: 54}\n",
    "3. Fetch users with age greater than or equal to 34. (2 marks)\n",
    "4. Update record {name:Abhishek } with age 34. (2 marks)\n",
    "5. Delete the record {name: Ravi }. (2 marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12952d7e-30ec-44c3-936d-b542da27aad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Create a collection named users. (2 mark)\n",
    "db.createCollection(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59bd7e0b-3edb-45b5-9d51-d049e5a1839b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Insert below records in users. (2 marks)\n",
    "\n",
    "db.users.insertMany([\n",
    "  { name: \"Abhishek\" },\n",
    "  { name: \"Raja\", age: 24 },\n",
    "  { name: \"Ravi\", age: 34 },\n",
    "  { name: \"Ram\", age: 45 },\n",
    "  { name: \"Roopa\", age: 44 },\n",
    "  { name: \"Tina\", age: 54 }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb11077c-4eb8-4bcf-be7f-42f20b527855",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " # Fetch users with age ≥ 34:\n",
    "\n",
    "\n",
    " db.users.find({ age: { $gte: 34 } })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7676f76-0942-46f6-9003-7213efc2702d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Update the record { name: \"Abhishek\" } by adding age: 34:\n",
    "\n",
    "db.users.updateOne(\n",
    "  { name: \"Abhishek\" },\n",
    "  { $set: { age: 34 } }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f571ce4-5630-4704-9ee6-c3b3e959f4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the record { name: \"Ravi\" }:\n",
    "\n",
    "db.users.deleteOne({ name: \"Ravi\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e65bce5e-0b77-44a8-b273-4cd3790e6f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write commands/query in MongoDB to,\n",
    "1. Create a collection named orders. (2 mark)\n",
    "2. Insert below two records in orders. (4 mark)\n",
    "{\"order_id”: 1,\n",
    "\"order_customer_id”: 11599,\n",
    "\"order_status”: \"CLOSED\" }\n",
    "{\"order_id”: 2,\n",
    "\"order_customer_id”: 11698,\n",
    "\"order_status”: \"OPEN\" }\n",
    "3. Fetch orders with order_status as COMPLETE. (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90ace1e-82b4-4ad5-a85d-0cfa57e9f654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a collection named orders. (2 mark)\n",
    "db.createCollection(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "815d2aeb-61be-4b47-8de2-ecdeb408922a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Insert below two records in orders. (4 mark) {\"order_id”: 1, \"order_customer_id”: 11599, \"order_status”: \"CLOSED\" } {\"order_id”: 2, \"order_customer_id”: 11698, \"order_status”: \"OPEN\" }\n",
    "db.orders.insertMany([\n",
    "  { order_id: 1, order_customer_id: 11599, order_status: \"CLOSED\" },\n",
    "  { order_id: 2, order_customer_id: 11698, order_status: \"OPEN\" }\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c1402e3-5e22-4301-86d7-abecfd3d6423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Fetch orders with order_status as COMPLETE. (4 marks)\n",
    "db.orders.find({ order_status: \"COMPLETE\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff104499-07e4-447d-af72-a668af6a89dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae543a3d-1a2a-4e08-9926-7adc12aa4338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be746701-1a47-42f6-acf2-d23d28964e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ESA Exam",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
